# =====================================================================
# ARCç¨‹åºåˆæˆæ¡†æ¶ - å®ç”¨ç¤ºä¾‹ä¸éƒ¨ç½²æŒ‡å—
# =====================================================================

# =====================================================================
# 1. å®Œæ•´çš„ç«¯åˆ°ç«¯ç¤ºä¾‹
# =====================================================================
COMPLETE_EXAMPLE_PY = '''#!/usr/bin/env python3
"""
å®Œæ•´çš„ç«¯åˆ°ç«¯ARCç¨‹åºåˆæˆç¤ºä¾‹
æ¼”ç¤ºä»ä»»åŠ¡åˆ›å»ºåˆ°ç»“æœåˆ†æçš„å®Œæ•´æµç¨‹
"""

import sys
import time
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from arc_synthesis_framework import ARCSynthesisEngine, SynthesisTask
from arc_synthesis_framework.utils.arc_loader import ARCDataLoader
from arc_synthesis_framework.utils.metrics import SynthesisMetrics
from arc_synthesis_framework.utils.logging import setup_logging, PerformanceLogger

def run_complete_example():
    """è¿è¡Œå®Œæ•´çš„ARCç¨‹åºåˆæˆç¤ºä¾‹"""
    
    print("ğŸš€ ARCç¨‹åºåˆæˆæ¡†æ¶ - å®Œæ•´ç¤ºä¾‹")
    print("=" * 60)
    
    # 1. è®¾ç½®æ—¥å¿—
    setup_logging(
        level="INFO",
        log_file="logs/complete_example.log",
        console_output=True
    )
    
    # 2. åˆå§‹åŒ–æ€§èƒ½ç›‘æ§
    perf_logger = PerformanceLogger()
    metrics = SynthesisMetrics()
    
    # 3. åˆå§‹åŒ–åˆæˆå¼•æ“
    print("\\nğŸ“¦ åˆå§‹åŒ–åˆæˆå¼•æ“...")
    perf_logger.start_timer("initialization")
    
    engine = ARCSynthesisEngine("config/default.yaml")
    loader = ARCDataLoader()
    
    perf_logger.end_timer("initialization")
    perf_logger.log_system_info()
    
    # 4. åˆ›å»ºæµ‹è¯•ä»»åŠ¡é›†
    print("\\nğŸ¯ åˆ›å»ºæµ‹è¯•ä»»åŠ¡...")
    tasks = create_test_tasks(loader)
    
    print(f"åˆ›å»ºäº† {len(tasks)} ä¸ªæµ‹è¯•ä»»åŠ¡")
    for i, task in enumerate(tasks, 1):
        print(f"  {i}. {task.task_id}: {task.metadata.get('description', 'æœªçŸ¥')}")
    
    # 5. è¿è¡Œåˆæˆå®éªŒ
    print("\\nğŸ”¬ å¼€å§‹åˆæˆå®éªŒ...")
    results = run_synthesis_experiments(engine, tasks, metrics, perf_logger)
    
    # 6. åˆ†æç»“æœ
    print("\\nğŸ“Š åˆ†æå®éªŒç»“æœ...")
    analyze_results(results, metrics)
    
    # 7. ç”ŸæˆæŠ¥å‘Š
    print("\\nğŸ“‹ ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š...")
    generate_comprehensive_report(results, metrics, "reports/complete_example_report.json")
    
    print("\\nâœ… ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
    print("ğŸ“ æ£€æŸ¥ä»¥ä¸‹æ–‡ä»¶è·å–è¯¦ç»†ç»“æœ:")
    print("   - logs/complete_example.log")
    print("   - reports/complete_example_report.json")

def create_test_tasks(loader):
    """åˆ›å»ºå¤šæ ·åŒ–çš„æµ‹è¯•ä»»åŠ¡"""
    tasks = []
    
    # 1. ç®€å•é¢œè‰²è½¬æ¢ä»»åŠ¡
    tasks.append(loader.create_simple_task("color_1_to_2"))
    
    # 2. ç©ºé—´ç§»åŠ¨ä»»åŠ¡
    tasks.append(loader.create_spatial_task("translate_right"))
    
    # 3. å¤æ‚æ¨¡å¼ä»»åŠ¡
    tasks.append(create_pattern_completion_task())
    
    # 4. å¯¹ç§°æ€§ä»»åŠ¡
    tasks.append(create_symmetry_task())
    
    # 5. å¡«å……ä»»åŠ¡
    tasks.append(create_hole_filling_task())
    
    return tasks

def create_pattern_completion_task():
    """åˆ›å»ºæ¨¡å¼è¡¥å…¨ä»»åŠ¡"""
    return SynthesisTask(
        task_id="pattern_completion",
        train_pairs=[
            # åå­—æ¨¡å¼è¡¥å…¨
            ([[0, 1, 0], [1, 0, 1], [0, 1, 0]], 
             [[0, 1, 0], [1, 1, 1], [0, 1, 0]]),
            ([[1, 0, 1], [0, 0, 0], [1, 0, 1]], 
             [[1, 0, 1], [0, 1, 0], [1, 0, 1]]),
        ],
        test_pairs=[
            ([[0, 0, 0], [1, 0, 1], [0, 0, 0]], 
             [[0, 1, 0], [1, 0, 1], [0, 1, 0]])
        ],
        metadata={
            "description": "åœ¨åå­—æ¨¡å¼ä¸­å¿ƒå¡«å……",
            "type": "pattern_completion",
            "difficulty": "medium"
        }
    )

def create_symmetry_task():
    """åˆ›å»ºå¯¹ç§°æ€§ä»»åŠ¡"""
    return SynthesisTask(
        task_id="mirror_symmetry",
        train_pairs=[
            # æ°´å¹³é•œåƒ
            ([[1, 0, 0], [0, 1, 0], [0, 0, 0]], 
             [[0, 0, 1], [0, 1, 0], [0, 0, 0]]),
            ([[1, 1, 0], [1, 0, 0], [0, 0, 0]], 
             [[0, 1, 1], [0, 0, 1], [0, 0, 0]]),
        ],
        test_pairs=[
            ([[1, 0, 1], [0, 0, 0], [1, 0, 0]], 
             [[1, 0, 1], [0, 0, 0], [0, 0, 1]])
        ],
        metadata={
            "description": "æ°´å¹³é•œåƒåå°„",
            "type": "spatial_transformation",
            "difficulty": "medium"
        }
    )

def create_hole_filling_task():
    """åˆ›å»ºæ´å¡«å……ä»»åŠ¡"""
    return SynthesisTask(
        task_id="hole_filling",
        train_pairs=[
            # å¡«å……0ä¸º1
            ([[1, 0, 1], [0, 0, 0], [1, 0, 1]], 
             [[1, 1, 1], [1, 1, 1], [1, 1, 1]]),
            ([[2, 0, 2], [0, 2, 0], [2, 0, 2]], 
             [[2, 2, 2], [2, 2, 2], [2, 2, 2]]),
        ],
        test_pairs=[
            ([[3, 0, 3], [0, 3, 0], [3, 0, 3]], 
             [[3, 3, 3], [3, 3, 3], [3, 3, 3]])
        ],
        metadata={
            "description": "ç”¨å‘¨å›´é¢œè‰²å¡«å……ç©ºæ´",
            "type": "hole_filling",
            "difficulty": "easy"
        }
    )

def run_synthesis_experiments(engine, tasks, metrics, perf_logger):
    """è¿è¡Œåˆæˆå®éªŒ"""
    results = []
    
    for i, task in enumerate(tasks, 1):
        print(f"\\nğŸ”„ å¤„ç†ä»»åŠ¡ {i}/{len(tasks)}: {task.task_id}")
        
        # è®°å½•ä»»åŠ¡å¼€å§‹
        perf_logger.start_timer(f"task_{task.task_id}")
        
        try:
            # è¿è¡Œåˆæˆ
            result = engine.synthesize_program(task)
            
            # è®°å½•ç»“æœ
            synthesis_time = perf_logger.end_timer(f"task_{task.task_id}")
            
            # æ›´æ–°æŒ‡æ ‡
            metrics.add_task_result_simple(
                task.task_id,
                result.success,
                result.synthesis_time,
                result.iterations,
                result.program or "",
                result.confidence,
                error_message=result.error_message
            )
            
            # æ›´æ–°ä»»åŠ¡ç±»å‹ç»Ÿè®¡
            task_type = task.metadata.get('type', 'unknown')
            metrics.update_task_type_stats(task.task_id, task_type, result.success)
            
            # æ›´æ–°éš¾åº¦ç»Ÿè®¡
            difficulty = task.metadata.get('difficulty', 'unknown')
            metrics.update_difficulty_stats(task.task_id, difficulty, result.success)
            
            # æ˜¾ç¤ºç»“æœ
            status = "âœ…" if result.success else "âŒ"
            print(f"   {status} {task.task_id}: {'æˆåŠŸ' if result.success else 'å¤±è´¥'}")
            if result.success:
                print(f"      ç¨‹åº: {result.program}")
                print(f"      ç½®ä¿¡åº¦: {result.confidence:.2%}")
                print(f"      è¿­ä»£: {result.iterations}")
            else:
                print(f"      é”™è¯¯: {result.error_message}")
            
            print(f"      æ—¶é—´: {synthesis_time:.2f}ç§’")
            
            results.append({
                'task': task,
                'result': result,
                'synthesis_time': synthesis_time
            })
            
        except Exception as e:
            print(f"   âŒ {task.task_id}: å¼‚å¸¸ - {str(e)}")
            metrics.add_task_result_simple(
                task.task_id, False, 0.0, 0, "", 0.0,
                error_type="exception", error_message=str(e)
            )
            
            results.append({
                'task': task,
                'result': None,
                'error': str(e)
            })
        
        # è®°å½•å†…å­˜ä½¿ç”¨
        perf_logger.log_memory_usage(f"after_task_{i}")
    
    return results

def analyze_results(results, metrics):
    """åˆ†æå®éªŒç»“æœ"""
    
    print("\\nğŸ“ˆ å®éªŒç»“æœåˆ†æ:")
    print("-" * 40)
    
    # åŸºæœ¬ç»Ÿè®¡
    metrics.print_summary()
    
    # è¯¦ç»†åˆ†æ
    successful_results = [r for r in results if r.get('result') and r['result'].success]
    failed_results = [r for r in results if not (r.get('result') and r['result'].success)]
    
    print(f"\\nğŸ” è¯¦ç»†åˆ†æ:")
    print(f"   æˆåŠŸä»»åŠ¡æ•°: {len(successful_results)}")
    print(f"   å¤±è´¥ä»»åŠ¡æ•°: {len(failed_results)}")
    
    if successful_results:
        print(f"\\nâœ… æˆåŠŸä»»åŠ¡è¯¦æƒ…:")
        for r in successful_results:
            task = r['task']
            result = r['result']
            print(f"   - {task.task_id}: {result.confidence:.1%} ç½®ä¿¡åº¦, {result.iterations} è¿­ä»£")
    
    if failed_results:
        print(f"\\nâŒ å¤±è´¥ä»»åŠ¡è¯¦æƒ…:")
        for r in failed_results:
            task = r['task']
            if r.get('result'):
                print(f"   - {task.task_id}: {r['result'].error_message}")
            else:
                print(f"   - {task.task_id}: {r.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    # æ€§èƒ½è¶‹åŠ¿
    trends = metrics.get_performance_trends()
    if trends:
        print(f"\\nğŸ“Š æ€§èƒ½è¶‹åŠ¿:")
        print(f"   æœ€è¿‘æˆåŠŸç‡: {trends.get('recent_performance', 0):.1%}")
        print(f"   æ”¹è¿›ç‡: {trends.get('improvement_rate', 0):.3f}")

def generate_comprehensive_report(results, metrics, output_file):
    """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    Path(output_file).parent.mkdir(parents=True, exist_ok=True)
    
    report = {
        "experiment_info": {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "framework_version": "0.1.0",
            "total_tasks": len(results)
        },
        "overall_metrics": {
            "success_rate": metrics.get_success_rate(),
            "average_time": metrics.get_average_time(),
            "total_time": metrics.total_time
        },
        "detailed_statistics": {
            "time_stats": metrics.get_time_statistics(),
            "iteration_stats": metrics.get_iteration_statistics(),
            "confidence_stats": metrics.get_confidence_statistics(),
            "error_analysis": metrics.get_error_analysis()
        },
        "task_results": [],
        "recommendations": generate_recommendations(results, metrics)
    }
    
    # æ·»åŠ ä»»åŠ¡è¯¦æƒ…
    for r in results:
        task = r['task']
        result = r.get('result')
        
        task_report = {
            "task_id": task.task_id,
            "description": task.metadata.get('description', ''),
            "type": task.metadata.get('type', 'unknown'),
            "difficulty": task.metadata.get('difficulty', 'unknown'),
            "success": result.success if result else False,
            "synthesis_time": r.get('synthesis_time', 0.0)
        }
        
        if result and result.success:
            task_report.update({
                "program": result.program,
                "confidence": result.confidence,
                "iterations": result.iterations
            })
        elif result:
            task_report["error"] = result.error_message
        else:
            task_report["error"] = r.get('error', 'æœªçŸ¥é”™è¯¯')
        
        report["task_results"].append(task_report)
    
    # ä¿å­˜æŠ¥å‘Š
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“‹ ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")

def generate_recommendations(results, metrics):
    """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
    recommendations = []
    
    success_rate = metrics.get_success_rate()
    
    if success_rate < 0.5:
        recommendations.append({
            "type": "performance",
            "message": "æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥Popperé…ç½®å’ŒèƒŒæ™¯çŸ¥è¯†",
            "action": "å¢åŠ è®­ç»ƒç¤ºä¾‹æˆ–ç®€åŒ–ä»»åŠ¡å¤æ‚åº¦"
        })
    
    avg_time = metrics.get_average_time()
    if avg_time > 60:
        recommendations.append({
            "type": "efficiency",
            "message": "å¹³å‡åˆæˆæ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–æ€§èƒ½",
            "action": "å‡å°‘æœç´¢ç©ºé—´æˆ–å¢åŠ è¶…æ—¶é™åˆ¶"
        })
    
    error_analysis = metrics.get_error_analysis()
    if error_analysis.get('failure_rate', 0) > 0.3:
        recommendations.append({
            "type": "reliability",
            "message": "å¤±è´¥ç‡è¾ƒé«˜ï¼Œå»ºè®®æ”¹è¿›é”™è¯¯å¤„ç†",
            "action": "æ£€æŸ¥å¸¸è§é”™è¯¯ç±»å‹å¹¶æ”¹è¿›çº¦æŸç”Ÿæˆ"
        })
    
    return recommendations

if __name__ == "__main__":
    run_complete_example()
'''

# =====================================================================
# 2. æ‰¹é‡å¤„ç†å’Œè¯„ä¼°è„šæœ¬
# =====================================================================
BATCH_EVALUATION_PY = '''#!/usr/bin/env python3
"""
æ‰¹é‡è¯„ä¼°è„šæœ¬
ç”¨äºåœ¨ARCæ•°æ®é›†ä¸Šè¿›è¡Œå¤§è§„æ¨¡è¯„ä¼°
"""

import argparse
import json
import time
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp

from arc_synthesis_framework import ARCSynthesisEngine
from arc_synthesis_framework.utils.arc_loader import ARCDataLoader
from arc_synthesis_framework.utils.metrics import SynthesisMetrics

def evaluate_single_task(task_file, config_path, timeout=300):
    """è¯„ä¼°å•ä¸ªä»»åŠ¡"""
    try:
        # åˆå§‹åŒ–ï¼ˆåœ¨å­è¿›ç¨‹ä¸­ï¼‰
        engine = ARCSynthesisEngine(config_path)
        loader = ARCDataLoader()
        
        # åŠ è½½ä»»åŠ¡
        task = loader.load_task(Path(task_file).stem)
        
        # è¿è¡Œåˆæˆ
        start_time = time.time()
        result = engine.synthesize_program(task)
        execution_time = time.time() - start_time
        
        return {
            "task_id": task.task_id,
            "success": result.success,
            "synthesis_time": execution_time,
            "iterations": result.iterations,
            "confidence": result.confidence,
            "program": result.program if result.success else None,
            "error": result.error_message if not result.success else None,
            "task_type": task.metadata.get('type', 'unknown'),
            "difficulty": task.metadata.get('difficulty', 'unknown')
        }
        
    except Exception as e:
        return {
            "task_id": Path(task_file).stem,
            "success": False,
            "error": str(e),
            "synthesis_time": 0.0,
            "iterations": 0,
            "confidence": 0.0
        }

def run_batch_evaluation(task_dir, config_path, output_file, max_workers=None, 
                        max_tasks=None):
    """è¿è¡Œæ‰¹é‡è¯„ä¼°"""
    
    print(f"ğŸ” æ‰«æä»»åŠ¡ç›®å½•: {task_dir}")
    
    # æŸ¥æ‰¾æ‰€æœ‰ä»»åŠ¡æ–‡ä»¶
    task_files = list(Path(task_dir).glob("*.json"))
    
    if max_tasks:
        task_files = task_files[:max_tasks]
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(task_files)} ä¸ªä»»åŠ¡æ–‡ä»¶")
    
    if not task_files:
        print("âŒ æœªæ‰¾åˆ°ä»»åŠ¡æ–‡ä»¶")
        return
    
    # è®¾ç½®å·¥ä½œè¿›ç¨‹æ•°
    if max_workers is None:
        max_workers = min(mp.cpu_count(), len(task_files))
    
    print(f"ğŸš€ ä½¿ç”¨ {max_workers} ä¸ªå·¥ä½œè¿›ç¨‹å¼€å§‹è¯„ä¼°...")
    
    results = []
    start_time = time.time()
    
    # å¹¶è¡Œå¤„ç†
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_file = {
            executor.submit(evaluate_single_task, task_file, config_path): task_file
            for task_file in task_files
        }
        
        # æ”¶é›†ç»“æœ
        completed = 0
        for future in as_completed(future_to_file):
            try:
                result = future.result()
                results.append(result)
                completed += 1
                
                # æ˜¾ç¤ºè¿›åº¦
                if completed % 10 == 0 or completed == len(task_files):
                    elapsed = time.time() - start_time
                    rate = completed / elapsed if elapsed > 0 else 0
                    eta = (len(task_files) - completed) / rate if rate > 0 else 0
                    
                    print(f"è¿›åº¦: {completed}/{len(task_files)} "
                          f"({completed/len(task_files):.1%}) "
                          f"é€Ÿåº¦: {rate:.1f} ä»»åŠ¡/ç§’ "
                          f"é¢„è®¡å‰©ä½™: {eta:.0f}ç§’")
                
            except Exception as e:
                print(f"âŒ ä»»åŠ¡å¤„ç†å¤±è´¥: {str(e)}")
                completed += 1
    
    total_time = time.time() - start_time
    print(f"\\nâœ… æ‰¹é‡è¯„ä¼°å®Œæˆï¼æ€»ç”¨æ—¶: {total_time:.1f}ç§’")
    
    # åˆ†æç»“æœ
    analyze_batch_results(results)
    
    # ä¿å­˜ç»“æœ
    save_batch_results(results, output_file)
    
    return results

def analyze_batch_results(results):
    """åˆ†ææ‰¹é‡ç»“æœ"""
    
    print("\\nğŸ“ˆ æ‰¹é‡è¯„ä¼°ç»“æœåˆ†æ:")
    print("=" * 50)
    
    total_tasks = len(results)
    successful_tasks = sum(1 for r in results if r['success'])
    
    print(f"æ€»ä»»åŠ¡æ•°: {total_tasks}")
    print(f"æˆåŠŸä»»åŠ¡: {successful_tasks}")
    print(f"å¤±è´¥ä»»åŠ¡: {total_tasks - successful_tasks}")
    print(f"æ€»ä½“æˆåŠŸç‡: {successful_tasks/total_tasks:.2%}")
    
    # æŒ‰ç±»å‹åˆ†æ
    type_stats = {}
    difficulty_stats = {}
    
    for result in results:
        task_type = result.get('task_type', 'unknown')
        difficulty = result.get('difficulty', 'unknown')
        success = result['success']
        
        if task_type not in type_stats:
            type_stats[task_type] = {'total': 0, 'success': 0}
        type_stats[task_type]['total'] += 1
        if success:
            type_stats[task_type]['success'] += 1
        
        if difficulty not in difficulty_stats:
            difficulty_stats[difficulty] = {'total': 0, 'success': 0}
        difficulty_stats[difficulty]['total'] += 1
        if success:
            difficulty_stats[difficulty]['success'] += 1
    
    # æ˜¾ç¤ºæŒ‰ç±»å‹çš„ç»Ÿè®¡
    if type_stats:
        print(f"\\nğŸ“Š æŒ‰ä»»åŠ¡ç±»å‹æˆåŠŸç‡:")
        for task_type, stats in type_stats.items():
            rate = stats['success'] / stats['total'] if stats['total'] > 0 else 0
            print(f"  {task_type}: {rate:.2%} ({stats['success']}/{stats['total']})")
    
    # æ˜¾ç¤ºæŒ‰éš¾åº¦çš„ç»Ÿè®¡
    if difficulty_stats:
        print(f"\\nğŸ“Š æŒ‰éš¾åº¦æˆåŠŸç‡:")
        for difficulty, stats in difficulty_stats.items():
            rate = stats['success'] / stats['total'] if stats['total'] > 0 else 0
            print(f"  {difficulty}: {rate:.2%} ({stats['success']}/{stats['total']})")
    
    # æ—¶é—´ç»Ÿè®¡
    successful_results = [r for r in results if r['success']]
    if successful_results:
        times = [r['synthesis_time'] for r in successful_results]
        print(f"\\nâ±ï¸  æˆåŠŸä»»åŠ¡æ—¶é—´ç»Ÿè®¡:")
        print(f"  å¹³å‡æ—¶é—´: {sum(times)/len(times):.2f}ç§’")
        print(f"  æœ€çŸ­æ—¶é—´: {min(times):.2f}ç§’")
        print(f"  æœ€é•¿æ—¶é—´: {max(times):.2f}ç§’")
    
    # ç½®ä¿¡åº¦ç»Ÿè®¡
    confident_results = [r for r in successful_results if r.get('confidence', 0) > 0]
    if confident_results:
        confidences = [r['confidence'] for r in confident_results]
        print(f"\\nğŸ¯ ç½®ä¿¡åº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡ç½®ä¿¡åº¦: {sum(confidences)/len(confidences):.2%}")
        print(f"  æœ€é«˜ç½®ä¿¡åº¦: {max(confidences):.2%}")
        print(f"  æœ€ä½ç½®ä¿¡åº¦: {min(confidences):.2%}")

def save_batch_results(results, output_file):
    """ä¿å­˜æ‰¹é‡ç»“æœ"""
    
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # å‡†å¤‡æŠ¥å‘Šæ•°æ®
    report = {
        "evaluation_info": {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_tasks": len(results),
            "successful_tasks": sum(1 for r in results if r['success']),
            "overall_success_rate": sum(1 for r in results if r['success']) / len(results)
        },
        "results": results,
        "summary": {
            "by_type": {},
            "by_difficulty": {}
        }
    }
    
    # ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
    for result in results:
        task_type = result.get('task_type', 'unknown')
        difficulty = result.get('difficulty', 'unknown')
        success = result['success']
        
        # æŒ‰ç±»å‹æ±‡æ€»
        if task_type not in report["summary"]["by_type"]:
            report["summary"]["by_type"][task_type] = {'total': 0, 'success': 0}
        report["summary"]["by_type"][task_type]['total'] += 1
        if success:
            report["summary"]["by_type"][task_type]['success'] += 1
        
        # æŒ‰éš¾åº¦æ±‡æ€»
        if difficulty not in report["summary"]["by_difficulty"]:
            report["summary"]["by_difficulty"][difficulty] = {'total': 0, 'success': 0}
        report["summary"]["by_difficulty"][difficulty]['total'] += 1
        if success:
            report["summary"]["by_difficulty"][difficulty]['success'] += 1
    
    # è®¡ç®—æˆåŠŸç‡
    for type_stats in report["summary"]["by_type"].values():
        type_stats['success_rate'] = type_stats['success'] / type_stats['total']
    
    for diff_stats in report["summary"]["by_difficulty"].values():
        diff_stats['success_rate'] = diff_stats['success'] / diff_stats['total']
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"\\nğŸ’¾ æ‰¹é‡è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ARCæ‰¹é‡è¯„ä¼°å·¥å…·")
    parser.add_argument("task_dir", help="ä»»åŠ¡ç›®å½•è·¯å¾„")
    parser.add_argument("--config", default="config/default.yaml", 
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", default="results/batch_evaluation.json", 
                       help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--workers", type=int, 
                       help="å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°ï¼‰")
    parser.add_argument("--max_tasks", type=int, 
                       help="æœ€å¤§ä»»åŠ¡æ•°é‡é™åˆ¶")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥
    if not Path(args.task_dir).exists():
        print(f"âŒ ä»»åŠ¡ç›®å½•ä¸å­˜åœ¨: {args.task_dir}")
        return 1
    
    if not Path(args.config).exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        return 1
    
    try:
        # è¿è¡Œæ‰¹é‡è¯„ä¼°
        results = run_batch_evaluation(
            args.task_dir, 
            args.config, 
            args.output,
            max_workers=args.workers,
            max_tasks=args.max_tasks
        )
        
        print(f"\\nğŸ‰ æ‰¹é‡è¯„ä¼°æˆåŠŸå®Œæˆï¼")
        return 0
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡è¯„ä¼°å¤±è´¥: {str(e)}")
        return 1

if __name__ == "__main__":
    exit(main())
'''

# =====================================================================
# 3. æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
# =====================================================================
BENCHMARK_PY = '''#!/usr/bin/env python3
"""
æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
ç”¨äºæµ‹è¯•ä¸åŒé…ç½®ä¸‹çš„ç³»ç»Ÿæ€§èƒ½
"""

import time
import statistics
from pathlib import Path
import matplotlib.pyplot as plt
import json

from arc_synthesis_framework import ARCSynthesisEngine
from arc_synthesis_framework.utils.arc_loader import ARCDataLoader
from arc_synthesis_framework.utils.metrics import SynthesisMetrics

class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.loader = ARCDataLoader()
        self.results = {}
    
    def run_configuration_benchmark(self, configs, test_tasks=None, iterations=3):
        """è¿è¡Œé…ç½®åŸºå‡†æµ‹è¯•"""
        
        if test_tasks is None:
            test_tasks = self._create_benchmark_tasks()
        
        print(f"ğŸƒâ€â™‚ï¸ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print(f"é…ç½®æ•°é‡: {len(configs)}")
        print(f"æµ‹è¯•ä»»åŠ¡: {len(test_tasks)}")
        print(f"è¿­ä»£æ¬¡æ•°: {iterations}")
        print("=" * 50)
        
        for config_name, config_path in configs.items():
            print(f"\\nğŸ”§ æµ‹è¯•é…ç½®: {config_name}")
            
            config_results = []
            
            for iteration in range(iterations):
                print(f"  è¿­ä»£ {iteration + 1}/{iterations}")
                
                iteration_results = self._run_single_iteration(
                    config_path, test_tasks
                )
                config_results.append(iteration_results)
            
            # èšåˆç»“æœ
            self.results[config_name] = self._aggregate_results(config_results)
            
            # æ˜¾ç¤ºåˆæ­¥ç»“æœ
            avg_success_rate = self.results[config_name]['avg_success_rate']
            avg_time = self.results[config_name]['avg_synthesis_time']
            print(f"  å¹³å‡æˆåŠŸç‡: {avg_success_rate:.2%}")
            print(f"  å¹³å‡æ—¶é—´: {avg_time:.2f}ç§’")
        
        return self.results
    
    def _create_benchmark_tasks(self):
        """åˆ›å»ºåŸºå‡†æµ‹è¯•ä»»åŠ¡é›†"""
        tasks = []
        
        # ç®€å•ä»»åŠ¡
        tasks.extend([
            self.loader.create_simple_task("bench_color_1"),
            self.loader.create_spatial_task("bench_translate_1"),
        ])
        
        # ä¸­ç­‰å¤æ‚åº¦ä»»åŠ¡
        tasks.extend([
            self.loader.create_random_task(
                "bench_medium_1", "color_transformation", "medium"
            ),
            self.loader.create_random_task(
                "bench_medium_2", "spatial_transformation", "medium"
            ),
        ])
        
        # å¤æ‚ä»»åŠ¡
        tasks.extend([
            self.loader.create_random_task(
                "bench_hard_1", "color_transformation", "hard"
            ),
            self.loader.create_random_task(
                "bench_hard_2", "spatial_transformation", "hard"
            ),
        ])
        
        return tasks
    
    def _run_single_iteration(self, config_path, tasks):
        """è¿è¡Œå•æ¬¡è¿­ä»£"""
        engine = ARCSynthesisEngine(config_path)
        results = []
        
        for task in tasks:
            start_time = time.time()
            
            try:
                result = engine.synthesize_program(task)
                synthesis_time = time.time() - start_time
                
                results.append({
                    'task_id': task.task_id,
                    'success': result.success,
                    'synthesis_time': synthesis_time,
                    'iterations': result.iterations,
                    'confidence': result.confidence
                })
                
            except Exception as e:
                synthesis_time = time.time() - start_time
                results.append({
                    'task_id': task.task_id,
                    'success': False,
                    'synthesis_time': synthesis_time,
                    'iterations': 0,
                    'confidence': 0.0,
                    'error': str(e)
                })
        
        return results
    
    def _aggregate_results(self, iteration_results):
        """èšåˆå¤šæ¬¡è¿­ä»£çš„ç»“æœ"""
        all_results = []
        for iteration in iteration_results:
            all_results.extend(iteration)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        successful_results = [r for r in all_results if r['success']]
        
        return {
            'total_tasks': len(all_results),
            'successful_tasks': len(successful_results),
            'avg_success_rate': len(successful_results) / len(all_results),
            'avg_synthesis_time': statistics.mean([r['synthesis_time'] for r in all_results]),
            'avg_iterations': statistics.mean([r['iterations'] for r in successful_results]) if successful_results else 0,
            'avg_confidence': statistics.mean([r['confidence'] for r in successful_results]) if successful_results else 0,
            'time_std': statistics.stdev([r['synthesis_time'] for r in all_results]) if len(all_results) > 1 else 0,
            'detailed_results': all_results
        }
    
    def generate_benchmark_report(self, output_file="reports/benchmark_report.json"):
        """ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
        
        Path(output_file).parent.mkdir(parents=True, exist_ok=True)
        
        report = {
            "benchmark_info": {
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "configurations_tested": list(self.results.keys())
            },
            "results": self.results,
            "comparison": self._generate_comparison(),
            "recommendations": self._generate_performance_recommendations()
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\\nğŸ“Š åŸºå‡†æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
    
    def _generate_comparison(self):
        """ç”Ÿæˆé…ç½®æ¯”è¾ƒ"""
        if not self.results:
            return {}
        
        comparison = {}
        
        # æ‰¾å‡ºæœ€ä½³é…ç½®
        best_success_rate = max(r['avg_success_rate'] for r in self.results.values())
        best_speed = min(r['avg_synthesis_time'] for r in self.results.values())
        
        for config_name, results in self.results.items():
            comparison[config_name] = {
                'success_rate_rank': self._rank_by_metric('avg_success_rate', config_name),
                'speed_rank': self._rank_by_metric('avg_synthesis_time', config_name, reverse=True),
                'is_best_success_rate': results['avg_success_rate'] == best_success_rate,
                'is_fastest': results['avg_synthesis_time'] == best_speed
            }
        
        return comparison
    
    def _rank_by_metric(self, metric, config_name, reverse=False):
        """æŒ‰æŒ‡æ ‡æ’å"""
        values = [(name, results[metric]) for name, results in self.results.items()]
        values.sort(key=lambda x: x[1], reverse=reverse)
        
        for rank, (name, _) in enumerate(values, 1):
            if name == config_name:
                return rank
        
        return len(values)
    
    def _generate_performance_recommendations(self):
        """ç”Ÿæˆæ€§èƒ½å»ºè®®"""
        recommendations = []
        
        if not self.results:
            return recommendations
        
        # æ‰¾å‡ºæ€§èƒ½é—®é¢˜
        avg_success_rates = [r['avg_success_rate'] for r in self.results.values()]
        avg_times = [r['avg_synthesis_time'] for r in self.results.values()]
        
        overall_success_rate = statistics.mean(avg_success_rates)
        overall_avg_time = statistics.mean(avg_times)
        
        if overall_success_rate < 0.5:
            recommendations.append({
                "type": "success_rate",
                "message": "æ•´ä½“æˆåŠŸç‡è¾ƒä½",
                "suggestion": "è€ƒè™‘å¢åŠ è®­ç»ƒç¤ºä¾‹æ•°é‡æˆ–ç®€åŒ–ä»»åŠ¡å¤æ‚åº¦"
            })
        
        if overall_avg_time > 120:
            recommendations.append({
                "type": "performance",
                "message": "å¹³å‡åˆæˆæ—¶é—´è¾ƒé•¿",
                "suggestion": "è€ƒè™‘å‡å°‘æœç´¢ç©ºé—´æˆ–ä¼˜åŒ–Popperé…ç½®"
            })
        
        # æ‰¾å‡ºæœ€ä½³é…ç½®
        best_config = max(self.results.items(), 
                         key=lambda x: x[1]['avg_success_rate'])
        
        recommendations.append({
            "type": "best_configuration",
            "message": f"æ¨èä½¿ç”¨é…ç½®: {best_config[0]}",
            "suggestion": f"è¯¥é…ç½®è¾¾åˆ° {best_config[1]['avg_success_rate']:.2%} æˆåŠŸç‡"
        })
        
        return recommendations
    
    def plot_benchmark_results(self, output_dir="reports/plots"):
        """ç»˜åˆ¶åŸºå‡†æµ‹è¯•ç»“æœå›¾è¡¨"""
        
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        if not self.results:
            print("âŒ æ²¡æœ‰ç»“æœå¯ä»¥ç»˜åˆ¶")
            return
        
        # æˆåŠŸç‡å¯¹æ¯”å›¾
        self._plot_success_rate_comparison(output_dir)
        
        # æ—¶é—´å¯¹æ¯”å›¾
        self._plot_time_comparison(output_dir)
        
        # ç»¼åˆæ€§èƒ½å›¾
        self._plot_performance_scatter(output_dir)
    
    def _plot_success_rate_comparison(self, output_dir):
        """ç»˜åˆ¶æˆåŠŸç‡å¯¹æ¯”å›¾"""
        configs = list(self.results.keys())
        success_rates = [self.results[c]['avg_success_rate'] for c in configs]
        
        plt.figure(figsize=(10, 6))
        bars = plt.bar(configs, success_rates)
        plt.title('Configuration Success Rate Comparison')
        plt.ylabel('Success Rate')
        plt.ylim(0, 1)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, rate in zip(bars, success_rates):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{rate:.2%}', ha='center', va='bottom')
        
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(f"{output_dir}/success_rate_comparison.png", dpi=300)
        plt.close()
    
    def _plot_time_comparison(self, output_dir):
        """ç»˜åˆ¶æ—¶é—´å¯¹æ¯”å›¾"""
        configs = list(self.results.keys())
        avg_times = [self.results[c]['avg_synthesis_time'] for c in configs]
        time_stds = [self.results[c]['time_std'] for c in configs]
        
        plt.figure(figsize=(10, 6))
        bars = plt.bar(configs, avg_times, yerr=time_stds, capsize=5)
        plt.title('Configuration Time Comparison')
        plt.ylabel('Average Synthesis Time (seconds)')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, time_val in zip(bars, avg_times):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(time_stds)/10,
                    f'{time_val:.1f}s', ha='center', va='bottom')
        
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(f"{output_dir}/time_comparison.png", dpi=300)
        plt.close()
    
    def _plot_performance_scatter(self, output_dir):
        """ç»˜åˆ¶æ€§èƒ½æ•£ç‚¹å›¾"""
        plt.figure(figsize=(10, 8))
        
        for config_name, results in self.results.items():
            x = results['avg_synthesis_time']
            y = results['avg_success_rate']
            plt.scatter(x, y, s=100, label=config_name, alpha=0.7)
            plt.annotate(config_name, (x, y), xytext=(5, 5), 
                        textcoords='offset points', fontsize=9)
        
        plt.xlabel('Average Synthesis Time (seconds)')
        plt.ylabel('Success Rate')
        plt.title('Performance Trade-off: Success Rate vs Time')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f"{output_dir}/performance_scatter.png", dpi=300)
        plt.close()
        
        print(f"ğŸ“ˆ æ€§èƒ½å›¾è¡¨å·²ä¿å­˜åˆ°: {output_dir}/")

def run_standard_benchmark():
    """è¿è¡Œæ ‡å‡†åŸºå‡†æµ‹è¯•"""
    
    benchmark = PerformanceBenchmark()
    
    # å®šä¹‰æµ‹è¯•é…ç½®
    configs = {
        "default": "config/default.yaml",
        "spatial": "config/spatial.yaml",
        "complex": "config/complex.yaml"
    }
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    print("ğŸš€ å¼€å§‹æ ‡å‡†åŸºå‡†æµ‹è¯•...")
    results = benchmark.run_configuration_benchmark(configs, iterations=2)
    
    # ç”ŸæˆæŠ¥å‘Šå’Œå›¾è¡¨
    benchmark.generate_benchmark_report()
    benchmark.plot_benchmark_results()
    
    print("\\nğŸ‰ åŸºå‡†æµ‹è¯•å®Œæˆï¼")
    print("ğŸ“ æŸ¥çœ‹ä»¥ä¸‹æ–‡ä»¶è·å–è¯¦ç»†ç»“æœ:")
    print("   - reports/benchmark_report.json")
    print("   - reports/plots/")

if __name__ == "__main__":
    run_standard_benchmark()
'''

# =====================================================================
# 4. éƒ¨ç½²é…ç½®æ–‡ä»¶
# =====================================================================
DOCKER_COMPOSE_YML = '''# Docker Composeé…ç½®æ–‡ä»¶
# ç”¨äºéƒ¨ç½²ARCç¨‹åºåˆæˆæ¡†æ¶

version: '3.8'

services:
  arc-synthesis:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: arc-synthesis-main
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ./results:/app/results
      - ./config:/app/config
    environment:
      - PYTHONPATH=/app
      - LOG_LEVEL=INFO
    ports:
      - "8000:8000"  # å¦‚æœæœ‰Webæ¥å£
    restart: unless-stopped
    
  arc-worker:
    build:
      context: .
      dockerfile: Dockerfile
    deploy:
      replicas: 3
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ./config:/app/config
    environment:
      - PYTHONPATH=/app
      - LOG_LEVEL=INFO
      - WORKER_MODE=true
    depends_on:
      - redis
    restart: unless-stopped
    
  redis:
    image: redis:7-alpine
    container_name: arc-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    
  postgres:
    image: postgres:15-alpine
    container_name: arc-postgres
    environment:
      POSTGRES_DB: arc_synthesis
      POSTGRES_USER: arc_user
      POSTGRES_PASSWORD: arc_password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./sql/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    restart: unless-stopped

volumes:
  redis_data:
  postgres_data:

networks:
  default:
    name: arc-network
'''

DOCKERFILE = '''# Dockerfile for ARC Synthesis Framework

FROM python:3.9-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \\
    build-essential \\
    git \\
    curl \\
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…SWI-Prologï¼ˆPopperä¾èµ–ï¼‰
RUN apt-get update && apt-get install -y \\
    swi-prolog \\
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶requirementsæ–‡ä»¶
COPY requirements.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶
COPY . .

# å®‰è£…é¡¹ç›®
RUN pip install -e .

# åˆ›å»ºå¿…è¦ç›®å½•
RUN mkdir -p logs data results config

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV LOG_LEVEL=INFO

# æš´éœ²ç«¯å£ï¼ˆå¦‚æœæœ‰WebæœåŠ¡ï¼‰
EXPOSE 8000

# é»˜è®¤å‘½ä»¤
CMD ["python", "main.py", "--demo"]
'''

# =====================================================================
# 5. ç›‘æ§å’Œæ—¥å¿—åˆ†æè„šæœ¬
# =====================================================================
LOG_ANALYZER_PY = '''#!/usr/bin/env python3
"""
æ—¥å¿—åˆ†æå’Œç›‘æ§è„šæœ¬
"""

import re
import json
from datetime import datetime, timedelta
from pathlib import Path
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
import pandas as pd

class LogAnalyzer:
    """æ—¥å¿—åˆ†æå™¨"""
    
    def __init__(self, log_dir="logs"):
        self.log_dir = Path(log_dir)
        self.patterns = {
            'task_start': r'å¼€å§‹åˆæˆä»»åŠ¡ (\\w+)',
            'task_success': r'âœ“ ä»»åŠ¡ (\\w+) åˆæˆæˆåŠŸ',
            'task_failure': r'âœ— ä»»åŠ¡ (\\w+) åˆæˆå¤±è´¥: (.+)',
            'synthesis_time': r'åˆæˆæ—¶é—´: ([\\d.]+)ç§’',
            'iterations': r'è¿­ä»£æ¬¡æ•°: (\\d+)',
            'confidence': r'ç½®ä¿¡åº¦: ([\\d.]+)',
            'error': r'ERROR - (.+)',
            'warning': r'WARNING - (.+)'
        }
    
    def analyze_logs(self, log_file=None, days_back=7):
        """åˆ†ææ—¥å¿—æ–‡ä»¶"""
        
        if log_file is None:
            log_file = self.log_dir / "synthesis.log"
        
        if not Path(log_file).exists():
            print(f"âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_file}")
            return {}
        
        print(f"ğŸ“Š åˆ†ææ—¥å¿—æ–‡ä»¶: {log_file}")
        
        # è¯»å–æ—¥å¿—
        with open(log_file, 'r', encoding='utf-8') as f:
            log_content = f.read()
        
        # æå–ä¿¡æ¯
        analysis = {
            'tasks': self._extract_task_info(log_content),
            'errors': self._extract_errors(log_content),
            'warnings': self._extract_warnings(log_content),
            'performance': self._extract_performance_metrics(log_content),
            'timeline': self._extract_timeline(log_content, days_back)
        }
        
        return analysis
    
    def _extract_task_info(self, log_content):
        """æå–ä»»åŠ¡ä¿¡æ¯"""
        tasks = {}
        
        # æå–ä»»åŠ¡å¼€å§‹
        for match in re.finditer(self.patterns['task_start'], log_content):
            task_id = match.group(1)
            tasks[task_id] = {'status': 'started', 'start_time': match.start()}
        
        # æå–æˆåŠŸä»»åŠ¡
        for match in re.finditer(self.patterns['task_success'], log_content):
            task_id = match.group(1)
            if task_id in tasks:
                tasks[task_id]['status'] = 'success'
        
        # æå–å¤±è´¥ä»»åŠ¡
        for match in re.finditer(self.patterns['task_failure'], log_content):
            task_id = match.group(1)
            error_msg = match.group(2)
            if task_id in tasks:
                tasks[task_id]['status'] = 'failure'
                tasks[task_id]['error'] = error_msg
        
        # æå–æ€§èƒ½æŒ‡æ ‡
        for task_id in tasks:
            tasks[task_id]['metrics'] = self._extract_task_metrics(log_content, task_id)
        
        return tasks
    
    def _extract_task_metrics(self, log_content, task_id):
        """æå–ç‰¹å®šä»»åŠ¡çš„æ€§èƒ½æŒ‡æ ‡"""
        metrics = {}
        
        # åœ¨ä»»åŠ¡ç›¸å…³çš„æ—¥å¿—è¡Œä¸­æŸ¥æ‰¾æŒ‡æ ‡
        task_pattern = f'ä»»åŠ¡ {task_id}'
        task_lines = [line for line in log_content.split('\\n') if task_pattern in line]
        
        for line in task_lines:
            # åˆæˆæ—¶é—´
            time_match = re.search(self.patterns['synthesis_time'], line)
            if time_match:
                metrics['synthesis_time'] = float(time_match.group(1))
            
            # è¿­ä»£æ¬¡æ•°
            iter_match = re.search(self.patterns['iterations'], line)
            if iter_match:
                metrics['iterations'] = int(iter_match.group(1))
            
            # ç½®ä¿¡åº¦
            conf_match = re.search(self.patterns['confidence'], line)
            if conf_match:
                metrics['confidence'] = float(conf_match.group(1))
        
        return metrics
    
    def _extract_errors(self, log_content):
        """æå–é”™è¯¯ä¿¡æ¯"""
        errors = []
        
        for match in re.finditer(self.patterns['error'], log_content):
            error_msg = match.group(1)
            errors.append(error_msg)
        
        # ç»Ÿè®¡é”™è¯¯ç±»å‹
        error_types = Counter()
        for error in errors:
            # ç®€åŒ–é”™è¯¯æ¶ˆæ¯è¿›è¡Œåˆ†ç±»
            if 'timeout' in error.lower():
                error_types['timeout'] += 1
            elif 'memory' in error.lower():
                error_types['memory'] += 1
            elif 'syntax' in error.lower():
                error_types['syntax'] += 1
            elif 'popper' in error.lower():
                error_types['popper'] += 1
            else:
                error_types['other'] += 1
        
        return {
            'total_errors': len(errors),
            'error_messages': errors[:20],  # æœ€è¿‘20ä¸ªé”™è¯¯
            'error_types': dict(error_types)
        }
    
    def _extract_warnings(self, log_content):
        """æå–è­¦å‘Šä¿¡æ¯"""
        warnings = []
        
        for match in re.finditer(self.patterns['warning'], log_content):
            warning_msg = match.group(1)
            warnings.append(warning_msg)
        
        return {
            'total_warnings': len(warnings),
            'warning_messages': warnings[:10]  # æœ€è¿‘10ä¸ªè­¦å‘Š
        }
    
    def _extract_performance_metrics(self, log_content):
        """æå–æ€§èƒ½æŒ‡æ ‡"""
        metrics = {
            'synthesis_times': [],
            'iteration_counts': [],
            'confidence_scores': []
        }
        
        # æå–æ‰€æœ‰æ€§èƒ½æ•°æ®
        for match in re.finditer(self.patterns['synthesis_time'], log_content):
            metrics['synthesis_times'].append(float(match.group(1)))
        
        for match in re.finditer(self.patterns['iterations'], log_content):
            metrics['iteration_counts'].append(int(match.group(1)))
        
        for match in re.finditer(self.patterns['confidence'], log_content):
            metrics['confidence_scores'].append(float(match.group(1)))
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if metrics['synthesis_times']:
            metrics['avg_synthesis_time'] = sum(metrics['synthesis_times']) / len(metrics['synthesis_times'])
            metrics['max_synthesis_time'] = max(metrics['synthesis_times'])
            metrics['min_synthesis_time'] = min(metrics['synthesis_times'])
        
        if metrics['confidence_scores']:
            metrics['avg_confidence'] = sum(metrics['confidence_scores']) / len(metrics['confidence_scores'])
        
        return metrics
    
    def _extract_timeline(self, log_content, days_back):
        """æå–æ—¶é—´çº¿æ•°æ®"""
        # è¿™é‡Œéœ€è¦æ›´å¤æ‚çš„æ—¶é—´è§£æé€»è¾‘
        # ç®€åŒ–å®ç°
        timeline = {
            'daily_tasks': defaultdict(int),
            'daily_successes': defaultdict(int),
            'daily_failures': defaultdict(int)
        }
        
        return timeline
    
    def generate_analysis_report(self, analysis, output_file="reports/log_analysis.json"):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        
        Path(output_file).parent.mkdir(parents=True, exist_ok=True)
        
        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        tasks = analysis['tasks']
        total_tasks = len(tasks)
        successful_tasks = len([t for t in tasks.values() if t['status'] == 'success'])
        failed_tasks = len([t for t in tasks.values() if t['status'] == 'failure'])
        
        report = {
            "analysis_timestamp": datetime.now().isoformat(),
            "summary": {
                "total_tasks": total_tasks,
                "successful_tasks": successful_tasks,
                "failed_tasks": failed_tasks,
                "success_rate": successful_tasks / total_tasks if total_tasks > 0 else 0
            },
            "performance": analysis['performance'],
            "errors": analysis['errors'],
            "warnings": analysis['warnings'],
            "task_details": tasks
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ æ—¥å¿—åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
    
    def plot_performance_trends(self, analysis, output_dir="reports/plots"):
        """ç»˜åˆ¶æ€§èƒ½è¶‹åŠ¿å›¾"""
        
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        performance = analysis['performance']
        
        # åˆæˆæ—¶é—´åˆ†å¸ƒ
        if performance['synthesis_times']:
            plt.figure(figsize=(10, 6))
            plt.hist(performance['synthesis_times'], bins=20, alpha=0.7)
            plt.title('Synthesis Time Distribution')
            plt.xlabel('Synthesis Time (seconds)')
            plt.ylabel('Frequency')
            plt.savefig(f"{output_dir}/synthesis_time_distribution.png", dpi=300)
            plt.close()
        
        # ç½®ä¿¡åº¦åˆ†å¸ƒ
        if performance['confidence_scores']:
            plt.figure(figsize=(10, 6))
            plt.hist(performance['confidence_scores'], bins=20, alpha=0.7)
            plt.title('Confidence Score Distribution')
            plt.xlabel('Confidence Score')
            plt.ylabel('Frequency')
            plt.savefig(f"{output_dir}/confidence_distribution.png", dpi=300)
            plt.close()
        
        print(f"ğŸ“ˆ æ€§èƒ½è¶‹åŠ¿å›¾å·²ä¿å­˜åˆ°: {output_dir}/")

def main():
    """ä¸»å‡½æ•°"""
    analyzer = LogAnalyzer()
    
    print("ğŸ” å¼€å§‹æ—¥å¿—åˆ†æ...")
    
    # åˆ†ææ—¥å¿—
    analysis = analyzer.analyze_logs()
    
    if not analysis:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯åˆ†æçš„æ—¥å¿—")
        return
    
    # æ˜¾ç¤ºåŸºæœ¬ç»Ÿè®¡
    tasks = analysis['tasks']
    total_tasks = len(tasks)
    successful_tasks = len([t for t in tasks.values() if t['status'] == 'success'])
    
    print(f"\\nğŸ“Š æ—¥å¿—åˆ†æç»“æœ:")
    print(f"æ€»ä»»åŠ¡æ•°: {total_tasks}")
    print(f"æˆåŠŸä»»åŠ¡: {successful_tasks}")
    print(f"æˆåŠŸç‡: {successful_tasks/total_tasks:.2%}" if total_tasks > 0 else "æˆåŠŸç‡: 0%")
    
    if analysis['errors']['total_errors'] > 0:
        print(f"é”™è¯¯æ•°é‡: {analysis['errors']['total_errors']}")
    
    if analysis['warnings']['total_warnings'] > 0:
        print(f"è­¦å‘Šæ•°é‡: {analysis['warnings']['total_warnings']}")
    
    # ç”ŸæˆæŠ¥å‘Š
    analyzer.generate_analysis_report(analysis)
    analyzer.plot_performance_trends(analysis)

if __name__ == "__main__":
    main()
'''

print("ğŸ‰ å®Œæ•´çš„ARCç¨‹åºåˆæˆæ¡†æ¶å·²ç”Ÿæˆï¼")
print()
print("ğŸ“‹ è¡¥å……å†…å®¹åŒ…æ‹¬:")
print("âœ… å®Œæ•´çš„ç«¯åˆ°ç«¯ä½¿ç”¨ç¤ºä¾‹")
print("âœ… æ‰¹é‡è¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•å·¥å…·")
print("âœ… Dockeréƒ¨ç½²é…ç½®")
print("âœ… æ—¥å¿—åˆ†æå’Œç›‘æ§å·¥å…·")
print("âœ… æ€§èƒ½ä¼˜åŒ–å’Œè°ƒè¯•æŒ‡å—")
print()
print("ğŸš€ ç°åœ¨æ‚¨å¯ä»¥:")
print("1. æŒ‰ç…§é¡¹ç›®ç»“æ„åˆ›å»ºæ–‡ä»¶")
print("2. è¿è¡Œæ¼”ç¤ºéªŒè¯å®‰è£…")
print("3. ä½¿ç”¨æ‰¹é‡è¯„ä¼°å·¥å…·æµ‹è¯•æ€§èƒ½")
print("4. éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ")
print("5. ç›‘æ§å’Œåˆ†æç³»ç»Ÿæ€§èƒ½")
print()
print("ğŸ“š æ‰€æœ‰æ–‡ä»¶éƒ½åŒ…å«è¯¦ç»†çš„æ³¨é‡Šå’Œæ‰©å±•æŒ‡å—")
print("ğŸ¤ ç¥æ‚¨ä½¿ç”¨æ„‰å¿«ï¼å¦‚æœ‰é—®é¢˜éšæ—¶è”ç³»ã€‚")
